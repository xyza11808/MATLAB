{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 â€” Poisson GLM\n",
    "\n",
    "This is a tutorial illustrating the fitting of a linear-Gaussian GLM\n",
    "(also known as linear least-squares regression model) and a Poisson GLM\n",
    "(aka  \"linear-nonlinear-Poisson\" model) to retinal ganglion cell spike\n",
    "trains stimulated with binary temporal white noise. \n",
    "\n",
    "(Data from [Uzzell & Chichilnisky, 2004](http://jn.physiology.org/content/92/2/780.long); see `README.txt` file in the `/data_RGCs` directory for details). \n",
    "\n",
    "*Last updated: Nov 9, 2016 (JW Pillow)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to use this tutorial:\n",
    "\n",
    "This is an interactive tutorial designed to walk you through the steps of\n",
    "fitting two classic models (linear-Gaussian GLM and Poisson GLM) to spike\n",
    "train data.  It is organized into 'cells' that can be executed one after\n",
    "the other using keyboard shortcuts.  The relevant shortcuts are:\n",
    "\n",
    "1. `Shift-Enter` - execute the current section and move down to the next\n",
    "2. `Ctrl-Enter` - execute the current section (without moving to the next section)\n",
    "\n",
    "In the following, I recommend positioning the figure window once it\n",
    "appears in a place where you can easily see it, with no other windows you need on top of it or underneath\n",
    "it. Each section of the tutorial will overwrite the figure window, so\n",
    "once you get it positioned correctly there will be no need to lift your\n",
    "hands from the keyboard. Just read the relevant code and make changes as desired,\n",
    "then use `Shift-Enter` to execute the section and move down\n",
    "to the next section. And repeat. The figure window will always display\n",
    "the plots made in the current section, so there's no need to go clicking\n",
    "through multiple windows to find the one you're looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we start let's run some magic commands to automatically save our progress once a second (with `%autosave 1`), force all graphics from the `matplotlib` package to be displayed inline (with `%matplotlib notebook`), and then import some of our favorite Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autosave 1\n",
    "%matplotlib tk\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ====  1. Load the raw data ============\n",
    "\n",
    "datdir = 'data_RGCs/Tutorial1_data.npz' # directory where stimulus lives\n",
    "Data = np.load(datdir)    \n",
    "\n",
    "Stim = Data['Stim']                     # stimulus (temporal binary white noise)\n",
    "stimtimes = Data['stimtimes']           # stim frame times in seconds (if desired)\n",
    "SpTimes = Data['SpTimes']               # load spike times (in units of stimulus frames)\n",
    "\n",
    "### Pick a cell to work with\n",
    "cellnum = 2                             # (0-1 are OFF cells; 2-3 are ON cells).\n",
    "tsp = SpTimes[cellnum]\n",
    "\n",
    "### Compute some basic statistics on the data\n",
    "dtStim = stimtimes[1]-stimtimes[0] # time bin size for stimulus (s)\n",
    "RefreshRate = 1/dtStim             # Refresh rate of the monitor\n",
    "nT = len(Stim)                     # number of time bins in stimulus\n",
    "nsp = len(tsp)                     # number of spikes\n",
    "\n",
    "### Print out some basic info\n",
    "print('--------------------------')\n",
    "print('Loaded RGC data: cell {}'.format(cellnum))\n",
    "print('Number of stim frames: {:d}  ({:.1f} minutes)'.format(nT, nT*dtStim/60))\n",
    "print('Time bin size: {:.1f} ms'.format(dtStim*1000))\n",
    "print('Number of spikes: {} (mean rate={:.1f} Hz)\\n'.format(nsp, nsp/nT*60))\n",
    "\n",
    "### Let's visualize some of the raw data\n",
    "plt.subplot(211)\n",
    "iiplot = np.arange(120) # bins of stimulus to plot\n",
    "ttplot = iiplot*dtStim  # time bins of stimulus\n",
    "plt.plot(ttplot,Stim[iiplot])\n",
    "plt.title('raw stimulus (full field flicker)')\n",
    "plt.ylabel('stim intensity')\n",
    "\n",
    "plt.subplot(212)\n",
    "tspplot = tsp[(tsp>=ttplot[0])&(tsp<ttplot[-1])]\n",
    "plt.plot(tspplot, [1]*len(tspplot), 'ko')\n",
    "plt.xlim([ttplot[0], ttplot[-1]]); plt.ylim([0,2])\n",
    "plt.title('spike times')\n",
    "plt.xlabel('time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ==== 2. Bin the spike train ===== \n",
    "\n",
    "# For now we will assume we want to use the same time bin size as the time\n",
    "# bins used for the stimulus. Later, though, we'll wish to vary this.\n",
    "\n",
    "tbins = np.arange(nT+1)*dtStim     # time bin centers for spike train binnning\n",
    "sps,_ = np.histogram(tsp,tbins)     # binned spike train\n",
    "\n",
    "### Replot the responses we'll putting into our regression as counts\n",
    "plt.subplot(212)\n",
    "plt.cla()\n",
    "plt.stem(ttplot,sps[iiplot])\n",
    "plt.title('binned spike counts')\n",
    "plt.ylabel('spike count')\n",
    "plt.xlabel('time (s)')\n",
    "plt.xlim([ttplot[0], ttplot[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ==== 3. Build the design matrix: slow version ======\n",
    "# This is a necessary step before we can fit the model: assemble a matrix\n",
    "# that contains the relevant regressors for each time bin of the response,\n",
    "# known as a design matrix.  Each row of this matrix contains the relevant\n",
    "# stimulus chunk for predicting the spike count at a given time bin\n",
    "\n",
    "### Set the number of time bins of stimulus to use for predicting spikes\n",
    "ntfilt = 25     # Try varying this, to see how performance changes!\n",
    "\n",
    "# Build the design matrix: Slow version\n",
    "paddedStim = np.hstack((np.zeros((ntfilt-1)), Stim))  # pad early bins of stimulus with zero\n",
    "Xdsgn = np.zeros((nT,ntfilt))\n",
    "for j in np.arange(nT):\n",
    "    Xdsgn[j] = paddedStim[j:j+ntfilt]                 # grab last 'nkt' bins of stmiulus and insert into this row\n",
    "\n",
    "    \n",
    "# Let's visualize a small part of the design matrix just to see it\n",
    "plt.clf() \n",
    "plt.imshow(Xdsgn[:50], aspect='auto', interpolation='nearest')\n",
    "plt.xlabel('lags before spike time')\n",
    "plt.ylabel('time bin of response')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Notice it has a structure where every row is a shifted copy of the row\n",
    "# above, which comes from the fact that for each time bin of response,\n",
    "# we're grabbing the preceding 'nkt' bins of stimulus as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ==== 3b. Build the design matrix: fast version =======\n",
    "\n",
    "# Of course there's a faster / more elegant way to do this. The design\n",
    "# matrix here is known as a 'Hankel' matrix, which is the same as a\n",
    "# Toeplitz matrix flipped left to right.\n",
    "\n",
    "### With no for loop\n",
    "from scipy.linalg import hankel\n",
    "\n",
    "paddedStim = np.hstack((np.zeros(ntfilt-1), Stim))   # pad early bins of stimulus with zero\n",
    "Xdsgn = hankel(paddedStim[:-ntfilt+1], Stim[-ntfilt:])\n",
    "\n",
    "# (You can check for you like that this gives the same matrix as the one created above!)\n",
    "\n",
    "plt.clf() \n",
    "plt.imshow(Xdsgn[:50], aspect='auto', interpolation='nearest')\n",
    "plt.xlabel('lags before spike time')\n",
    "plt.ylabel('time bin of response')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### === 4. Compute and visualize the spike-triggered average (STA) ====\n",
    "\n",
    "# When the stimulus is Gaussian white noise, the STA provides an unbiased\n",
    "# estimator for the filter in a GLM / LNP model (as long as the nonlinearity\n",
    "# results in an STA whose expectation is not zero; feel free \n",
    "# to ignore this parenthetical remark if you're not interested in technical\n",
    "# details. It just means that if the nonlinearity is symmetric, \n",
    "# eg. x^2, then this condition won't hold, and the STA won't be useful).\n",
    "\n",
    "# In many cases it's useful to visualize the STA (even if your stimuli are\n",
    "# not white noise), just because if we don't see any kind of structure then\n",
    "# this may indicate that we have a problem (e.g., a mismatch between the\n",
    "# design matrix and binned spike counts.\n",
    "\n",
    "### It's extremely easy to compute the STA now that we have the design matrix\n",
    "sta = (Xdsgn.T @ sps)/nsp\n",
    "\n",
    "### Plot it\n",
    "ttk = np.arange(-ntfilt+1,1)*dtStim  # time bins for STA (in seconds)\n",
    "plt.clf()\n",
    "plt.plot(ttk,ttk*0, 'k--')\n",
    "plt.plot(ttk, sta, 'bo-')\n",
    "plt.title('STA')\n",
    "plt.xlabel('time before spike (s)')\n",
    "plt.xlim([ttk[0],ttk[-1]])\n",
    "plt.show()\n",
    "\n",
    "# If you're still using cell #1, this should look like a biphasic filter\n",
    "# with a negative lobe just prior to the spike time.\n",
    "\n",
    "# (By contrast, if this looks like garbage then it's a good chance we did something wrong!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ===== 4b. whitened STA (ML fit to filter for linear-Gaussian GLM) ======\n",
    "\n",
    "# If the stimuli are non-white, then the STA is generally a biased\n",
    "# estimator for the linear filter. In this case we may wish to compute the\n",
    "# \"whitened\" STA, which is also the maximum-likelihood estimator for the filter of a \n",
    "# GLM with \"identity\" nonlinearity and Gaussian noise (also known as\n",
    "# least-squares regression).\n",
    "\n",
    "# If the stimuli have correlations this ML estimate may look like garbage\n",
    "# (more on this later when we come to \"regularization\").  But for this\n",
    "# dataset the stimuli are white, so we don't (in general) expect a big\n",
    "# difference from the STA.  (This is because the whitening matrix\n",
    "# (Xdsng.T * Xdsgn)^{-1} is close to a scaled version of the identity.)\n",
    "\n",
    "from numpy.linalg import inv, norm\n",
    "### whitened STA\n",
    "wsta = inv(Xdsgn.T @ Xdsgn) @ sta * nsp\n",
    "# this is just the least-squares regression formula!\n",
    "\n",
    "### Let's plot them both (rescaled as unit vectors so we can see differences in their shape).\n",
    "plt.clf()\n",
    "plt.plot(ttk,ttk*0, 'k--')\n",
    "plt.plot(ttk, sta/norm(sta), 'bo-', label=\"STA\")\n",
    "plt.plot(ttk, wsta/norm(wsta), 'ro-', label=\"wSTA\")\n",
    "plt.legend()\n",
    "plt.title('STA and whitened STA')\n",
    "plt.xlabel('time before spike (s)')\n",
    "plt.xlim([ttk[0],ttk[-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ===== 4c. Predicting spikes with a linear-Gaussian GLM ======\n",
    "\n",
    "# The whitened STA can actually be used to predict spikes because it\n",
    "# corresponds to a proper estimate of the model parameters (i.e., for a\n",
    "# Gaussian GLM). Let's inspect this prediction\n",
    "\n",
    "sppred_lgGLM = Xdsgn @ wsta  # predicted spikes from linear-Gaussian GLM\n",
    "\n",
    "# Let's see how good this \"prediction\" is\n",
    "# (Prediction in quotes because we are (for now) looking at the performance\n",
    "# on training data, not test data... so it isn't really aprediction!)\n",
    "\n",
    "### Plot real spike train and prediction\n",
    "plt.clf()\n",
    "markerline,_,_ = plt.stem(ttplot,sps[iiplot], linefmt='b-', basefmt='k-', label=\"spike ct\")\n",
    "plt.setp(markerline, 'markerfacecolor', 'none')\n",
    "plt.setp(markerline, 'markeredgecolor', 'blue')\n",
    "plt.plot(ttplot,sppred_lgGLM[iiplot], color='red', linewidth=2, label=\"lgGLM\")\n",
    "plt.title('linear-Gaussian GLM: spike count prediction')\n",
    "plt.ylabel('spike count'); plt.xlabel('time (s)')\n",
    "plt.xlim([ttplot[0], ttplot[-1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ===== 4d. Fitting and predicting with a linear-Gaussian-GLM with offset =====\n",
    "\n",
    "# Oops, one thing we forgot above was to include an offset or \"constant\"\n",
    "# term in the design matrix, which will allow our prediction to have\n",
    "# non-zero mean (since the stimulus here was normalized to have zero mean).\n",
    "\n",
    "### Updated design matrix\n",
    "Xdsgn2 = np.hstack((np.ones((nT,1)), Xdsgn))     # just add a column of ones\n",
    "\n",
    "### Compute whitened STA\n",
    "MLwts = inv(Xdsgn2.T @ Xdsgn2) @ (Xdsgn2.T @ sps)  # this is just the LS regression formula\n",
    "const = MLwts[0]   # the additive constant\n",
    "wsta2 = MLwts[1:]  # the linear filter part\n",
    "\n",
    "### Now redo prediction (with offset)\n",
    "sppred_lgGLM2 = const + Xdsgn @ wsta2\n",
    "\n",
    "### Plot this stuff\n",
    "plt.clf()\n",
    "markerline,_,_ = plt.stem(ttplot,sps[iiplot], linefmt='b-', label=\"spike ct\")\n",
    "plt.setp(markerline, 'markerfacecolor', 'none')\n",
    "plt.setp(markerline, 'markeredgecolor', 'blue')\n",
    "plt.plot(ttplot,sppred_lgGLM[iiplot], color=\"red\", linewidth=2, label = 'lgGLM')\n",
    "plt.plot(ttplot,sppred_lgGLM2[iiplot], color=\"gold\", linewidth=2, label = 'lgGLM w/ offset') \n",
    "plt.title('linear-Gaussian GLM: spike count prediction')\n",
    "plt.ylabel('spike count'); plt.xlabel('time (s)')\n",
    "plt.xlim([ttplot[0], ttplot[-1]])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### Let's report the relevant training error (squared prediction error on\n",
    "### training data) so far just to see how we're doing:\n",
    "mse1 = np.mean((sps-sppred_lgGLM)**2)   # mean squared error, GLM no offset\n",
    "mse2 = np.mean((sps-sppred_lgGLM2)**2)  # mean squared error, with offset\n",
    "rss = np.mean((sps-np.mean(sps))**2)    # squared error of spike train\n",
    "print('Training perf (R^2): lin-gauss GLM, no offset: {:.2f}'.format(1-mse1/rss))\n",
    "print('Training perf (R^2): lin-gauss GLM, w/ offset: {:.2f}'.format(1-mse2/rss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ======  5. Poisson GLM ====================\n",
    "\n",
    "# Let's finally move on to the LNP / Poisson GLM!\n",
    "\n",
    "# Package available for download from\n",
    "# https://github.com/pavanramkumar/pyglmnet\n",
    "from pyglmnet import GLM\n",
    "\n",
    "### This is super-easy if we rely on built-in GLM fitting code\n",
    "reg_lambda = np.logspace(np.log(1e-6), np.log(1e-8), 100, base=np.exp(1))\n",
    "glm_poissonexp = GLM(distr='poisson', verbose=False, alpha=0.05,\n",
    "            max_iter=1000, learning_rate=2e-1, score_metric='pseudo_R2',\n",
    "            reg_lambda=reg_lambda, eta=4.0)\n",
    "\n",
    "glm_poissonexp.fit(Xdsgn, sps)\n",
    "\n",
    "pGLMconst = glm_poissonexp[-1].fit_['beta0']\n",
    "pGLMfilt = glm_poissonexp[-1].fit_['beta']\n",
    "\n",
    "# The 'GLM' function can fit a GLM for us. Here we have specified that\n",
    "# we want the noise model to be Poisson. The default setting for the link\n",
    "# function (the inverse of the nonlinearity) is 'log', so default\n",
    "# nonlinearity is 'exp').  \n",
    "\n",
    "### Compute predicted spike rate on training data\n",
    "ratepred_pGLM = np.exp(pGLMconst + Xdsgn @ pGLMfilt)\n",
    "# equivalent to if we had just written np.exp(Xdsgn2 @ pGLMwts)/dtStim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ===== 5b. Make plots showing and spike rate predictions ======\n",
    "\n",
    "plt.clf()\n",
    "plt.subplot(211)\n",
    "plt.plot(ttk,ttk*0, 'k--')\n",
    "plt.plot(ttk, wsta2/norm(wsta2), 'o-', label='lgGLM filt')\n",
    "plt.plot(ttk, pGLMfilt/norm(pGLMfilt), 'o-', label='pGLM filt')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title('(normalized) linear-Gaussian and Poisson GLM filter estimates')\n",
    "plt.xlabel('time before spike (s)')\n",
    "plt.xlim([ttk[0], ttk[-1]])\n",
    "\n",
    "plt.subplot(212)\n",
    "markerline,_,_ = plt.stem(ttplot,sps[iiplot], label=\"spike ct\")\n",
    "plt.setp(markerline, 'markerfacecolor', 'none')\n",
    "plt.setp(markerline, 'markeredgecolor', 'blue')\n",
    "plt.plot(ttplot,sppred_lgGLM2[iiplot], label=\"lgGLM\")\n",
    "plt.plot(ttplot,ratepred_pGLM[iiplot], label=\"pGLM\") \n",
    "plt.title('spike count / rate predictions')\n",
    "plt.ylabel('spike count / bin'); plt.xlabel('time (s)')\n",
    "plt.xlim([ttplot[0], ttplot[-1]])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Note the rate prediction here is in units of spikes/bin. If we wanted\n",
    "# spikes/sec, we could divide it by bin size dtStim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Do non-parametric estimate of the nonlinearity. \n",
    "\n",
    "# (Last piece still to do!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lastly: go back and try it out for the other three neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
